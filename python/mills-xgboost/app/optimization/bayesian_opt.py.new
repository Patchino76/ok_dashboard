import pandas as pd
import numpy as np
from bayes_opt import BayesianOptimization
# In version 3.0.1, the acquisition functions are directly imported from the package
from bayes_opt import acquisition as bayes_acq
import logging
import json
import os
import sys
from datetime import datetime

# Standard logger
logger = logging.getLogger(__name__)

# Set up a dedicated logger for Bayesian optimization
bayes_logger = logging.getLogger('bayes_optimization')

# Check if the bayes logger already has handlers to avoid duplicates
if not bayes_logger.handlers:
    # Create logs directory if it doesn't exist
    logs_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'logs')
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
        
    # Create a dedicated log file handler for Bayesian optimization
    bayes_log_file = os.path.join(logs_dir, 'bayes_opt.log')
    file_handler = logging.FileHandler(bayes_log_file, mode='w')
    
    # Define a custom formatter that includes timestamp and level
    formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - BAYES-OPT: %(message)s')
    file_handler.setFormatter(formatter)
    
    # Add the handler to the logger
    bayes_logger.addHandler(file_handler)
    
    # Set the logging level
    bayes_logger.setLevel(logging.DEBUG)
    
    # Prevent propagation to avoid duplicate logs
    bayes_logger.propagate = False
    
    bayes_logger.info('Bayesian optimization logger initialized')

class MillBayesianOptimizer:
    """
    Bayesian optimization for mill parameter tuning.
    Uses a trained XGBoost model to find optimal parameter settings.
    """
    
    def __init__(self, xgboost_model, target_col='PSI80', maximize=True):
        """
        Initialize the Bayesian optimizer
        
        Args:
            xgboost_model: Trained XGBoost model instance
            target_col: Target column to optimize (default: 'PSI80')
            maximize: Whether to maximize (True) or minimize (False) the target
        """
        self.model = xgboost_model
        self.target_col = target_col
        self.maximize = maximize
        self.optimizer = None
        self.pbounds = {}
        self.param_constraints = {}
        self.best_params = None
        self.optimization_history = []
        
        logger.info(f"Bayesian optimizer initialized for {'maximizing' if maximize else 'minimizing'} {target_col}")
        
    def _black_box_function(self, **kwargs):
        """
        Black box function that predicts the outcome using the XGBoost model
        
        Args:
            **kwargs: Parameter values to optimize
            
        Returns:
            Predicted target value (negated if minimizing)
        """
        # Create a DataFrame with a single row containing the parameters
        test_data = pd.DataFrame([kwargs])
        
        # Make prediction with the model
        prediction = self.model.predict(test_data)[0]
        
        # If minimizing, negate the value for the optimizer (BayesOpt always maximizes)
        return prediction if self.maximize else -prediction
        
    def set_parameter_bounds(self, pbounds=None, data=None):
        """
        Set the parameter bounds for optimization
        
        Args:
            pbounds: Dictionary mapping parameter names to (min, max) tuples
            data: Optional DataFrame to derive bounds from
            
        Returns:
            Dictionary of parameter bounds
        """
        if pbounds:
            self.pbounds = pbounds
            return self.pbounds
            
        # Default bounds if none specified and no data provided
        default_bounds = {
            'Temperature': (150, 190),
            'Speed': (15, 30),
            'Pressure': (40, 80)
        }
        
        if data is not None:
            # Derive bounds from data
            self.pbounds = {}
            for col in data.columns:
                if col in ['Temperature', 'Speed', 'Pressure']:
                    # Use slightly wider bounds than observed in the data
                    min_val = data[col].min() * 0.9
                    max_val = data[col].max() * 1.1
                    self.pbounds[col] = (min_val, max_val)
        else:
            self.pbounds = default_bounds
            
        return self.pbounds
        
    def set_constraints(self, constraints=None):
        """
        Set constraints on parameter combinations
        
        Args:
            constraints: Dictionary of constraint functions
                        Each key is a constraint name, value is a function that
                        takes parameter dictionary and returns True if valid
        """
        if constraints:
            self.param_constraints = constraints
        else:
            # Example constraint: Temperature * Speed must be below 4500
            def temp_speed_constraint(params):
                return params.get('Temperature', 0) * params.get('Speed', 0) < 4500
                
            self.param_constraints = {
                'temp_speed_limit': temp_speed_constraint
            }
            
        return True
        
    def _check_constraints(self, params):
        """
        Check if parameters meet all constraints
        
        Args:
            params: Dictionary of parameter values
            
        Returns:
            True if all constraints are met, False otherwise
        """
        if not self.param_constraints:
            return True
            
        for name, constraint_func in self.param_constraints.items():
            if not constraint_func(params):
                return False
                
        return True
        
    def optimize(self, init_points=5, n_iter=25, acq='ei', kappa=2.5, xi=0.0, save_dir=None):
        """
        Run the Bayesian optimization process
        
        Args:
            init_points: Number of random exploration steps before exploitation
            n_iter: Number of optimization iterations
            acq: Acquisition function type ('ei', 'ucb', or 'poi')
            kappa: Parameter for 'ucb' acquisition function
            xi: Parameter for 'ei' and 'poi' acquisition functions
            save_dir: Optional directory to save optimization results
            
        Returns:
            Dictionary with optimization results or error information
        """
        # Log the start of optimization with detailed parameters
        bayes_logger.info("="*80)
        bayes_logger.info(f"STARTING BAYESIAN OPTIMIZATION FOR TARGET: {self.target_col}")
        bayes_logger.info(f"Optimization goal: {'MAXIMIZE' if self.maximize else 'MINIMIZE'} {self.target_col}")
        bayes_logger.info(f"Initial points: {init_points}, Optimization iterations: {n_iter}")
        bayes_logger.info(f"Acquisition function: {acq}, kappa: {kappa}, xi: {xi}")
        bayes_logger.info("="*80)
        
        try:
            # Check if parameter bounds are set
            if not self.pbounds:
                logger.warning("Parameter bounds not set, using defaults")
                self.set_parameter_bounds()
                
            # Log the parameter bounds being used
            bayes_logger.info("Parameter bounds:")
            for param, bounds in self.pbounds.items():
                bayes_logger.info(f"  {param}: {bounds}")
            
            # Initialize optimizer
            self.optimizer = BayesianOptimization(
                f=None,  # Will manually call _black_box_function
                pbounds=self.pbounds,
                verbose=2,  # Will use our own logging instead
                random_state=42
            )
            
            # Set acquisition function
            if acq == 'ucb':
                acq_func = bayes_acq.UtilityFunction(kind=acq, kappa=kappa)
            else:  # 'ei' or 'poi'
                acq_func = bayes_acq.UtilityFunction(kind=acq, xi=xi)
                
            self.optimizer.set_acq(acq_func)
            
            # Flag to track if any optimization step succeeded
            all_steps_failed = True
            
            # Default bad value to use when steps fail
            default_bad_value = -1e6 if self.maximize else 1e6
            
            # Define a wrapper function to safely register points and log the process
            def safe_register(params, target_value, iteration_type, iteration_num, total_iterations):
                """
                Safely register a point with the optimizer, with error handling and logging
                
                Args:
                    params: Dictionary of parameter values
                    target_value: Value of the target metric
                    iteration_type: String indicating the type of iteration ('EXPLORE' or 'OPTIMIZE')
                    iteration_num: Current iteration number
                    total_iterations: Total number of iterations
                    
                Returns:
                    Clipped target value
                """
                try:
                    # Clip target value to avoid extreme values causing numerical issues
                    clipped_target = max(min(target_value, 1e6), -1e6)
                    
                    # Register the point with the optimizer
                    self.optimizer.register(params=params, target=clipped_target)
                    
                    # Log the registration
                    param_str = ", ".join([f"{k}={v:.4f}" for k, v in params.items()])
                    bayes_logger.info(f"{iteration_type} step {iteration_num}/{total_iterations} - "
                                      f"Target: {clipped_target:.6f} - Params: {param_str}")
                    
                    # Check if this is a new best point
                    if hasattr(self.optimizer, 'max') and self.optimizer.max:
                        if self.optimizer.max['target'] == clipped_target:
                            bayes_logger.info(f"NEW BEST! Target: {clipped_target:.6f}")
                    
                    # Save the parameters and resulting target for history
                    history_entry = params.copy()
                    history_entry[self.target_col] = target_value
                    history_entry['iteration'] = iteration_num
                    history_entry['phase'] = iteration_type
                    self.optimization_history.append(history_entry)
                    
                    return clipped_target
                    
                except Exception as e:
                    error_msg = f"Error registering point: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
                    return default_bad_value
            
            # Phase 1: Random exploration
            bayes_logger.info("-"*60)
            bayes_logger.info("PHASE 1: EXPLORATION (Random Sampling)")
            bayes_logger.info("-"*60)
            
            for i in range(init_points):
                try:
                    # Random exploration
                    next_point = self.optimizer.suggest()
                    target = self._black_box_function(**next_point)
                    clipped_target = safe_register(next_point, target, "EXPLORE", i+1, init_points)
                    logger.info(f"Exploration step {i+1}/{init_points}, target: {clipped_target:.4f}")
                    
                    # At least one step succeeded
                    if clipped_target != default_bad_value:
                        all_steps_failed = False
                        
                except Exception as e:
                    error_msg = f"Error in exploration step {i+1}: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
            
            bayes_logger.info("-"*60)
            bayes_logger.info("PHASE 2: EXPLOITATION (Guided Optimization)")
            bayes_logger.info("-"*60)
            
            # Execute exploitation phase with guided optimization
            for i in range(n_iter):
                try:
                    # Guided optimization
                    next_point = self.optimizer.suggest()
                    target = self._black_box_function(**next_point)
                    clipped_target = safe_register(next_point, target, "OPTIMIZE", i+1, n_iter)
                    logger.info(f"Optimization step {i+1}/{n_iter}, target: {clipped_target:.4f}")
                    
                    # At least one step succeeded
                    if clipped_target != default_bad_value:
                        all_steps_failed = False
                        
                except Exception as e:
                    error_msg = f"Error in optimization step {i+1}: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
                    
            # If all steps failed, raise an exception
            if all_steps_failed:
                error_msg = "All optimization steps failed. Check the logs for details."
                logger.error(error_msg)
                bayes_logger.error(error_msg)
                bayes_logger.error("OPTIMIZATION PROCESS TERMINATED - ALL STEPS FAILED")
                raise ValueError(error_msg)
            
            # Check if we have valid optimization results
            if hasattr(self.optimizer, 'max') and self.optimizer.max:
                # Get the best parameters and target value
                self.best_params = self.optimizer.max['params']
                best_target = self.optimizer.max['target']
                
                # Calculate actual prediction for the best parameters
                best_prediction = best_target if self.maximize else -best_target
            else:
                # This shouldn't happen due to the all_steps_failed check above, but just in case
                error_msg = "No valid optimization results found"
                logger.error(error_msg)
                bayes_logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Log the optimization results in detail
            bayes_logger.info("-"*60)
            bayes_logger.info("OPTIMIZATION COMPLETE")
            bayes_logger.info("-"*60)
            bayes_logger.info(f"Best {self.target_col} value: {best_target:.6f} ({'maximized' if self.maximize else 'minimized'})")
            bayes_logger.info("Best parameter combination:")
            for param, value in self.best_params.items():
                bayes_logger.info(f"  {param}: {value:.4f}")
            
            # Show improvement over baseline if available
            if hasattr(self, 'baseline_performance') and self.baseline_performance is not None:
                improvement = best_target - self.baseline_performance if self.maximize else self.baseline_performance - best_target
                percent_improvement = (improvement / abs(self.baseline_performance)) * 100 if self.baseline_performance != 0 else float('inf')
                bayes_logger.info(f"Improvement over baseline: {improvement:.4f} ({percent_improvement:.2f}%)")
            
            # Save results if directory is provided
            if save_dir:
                self.save_results(save_dir)
                bayes_logger.info(f"Results saved to: {save_dir}")
            
            # Log the final recommendations
            recommendations = self.recommend_parameters(n_recommendations=3)
            bayes_logger.info("-"*60)
            bayes_logger.info("TOP RECOMMENDATIONS:")
            for i, rec in enumerate(recommendations):
                bayes_logger.info(f"Recommendation #{i+1} (Expected {self.target_col}: {rec['prediction']:.4f}):")
                for param, value in {k: v for k, v in rec.items() if k != 'prediction'}.items():
                    bayes_logger.info(f"  {param}: {value:.4f}")
            
            # Return optimization results
            return {
                "best_params": self.best_params,
                "best_target": best_target,
                "maximize": self.maximize,
                "actual_prediction": best_prediction,
                "recommendations": recommendations,
                "optimization_history": self.optimization_history
            }
        
        except Exception as e:
            error_msg = f"Optimization process failed: {str(e)}"
            logger.error(error_msg)
            bayes_logger.error(error_msg)
            bayes_logger.error("OPTIMIZATION PROCESS TERMINATED WITH ERROR")
            
            # Return error information
            return {
                "error": str(e),
                "optimization_history": self.optimization_history
            }
    
    def save_results(self, directory):
        """
        Save optimization results to disk
        
        Args:
            directory: Directory to save results
        """
        if not os.path.exists(directory):
            os.makedirs(directory)
            
        # Save optimization history
        if self.optimization_history:
            history_df = pd.DataFrame(self.optimization_history)
            history_path = os.path.join(directory, f"optimization_history_{self.target_col}.csv")
            history_df.to_csv(history_path, index=False)
            
        # Save best parameters
        if self.best_params:
            best_params_path = os.path.join(directory, f"best_params_{self.target_col}.json")
            with open(best_params_path, 'w') as f:
                json.dump(self.best_params, f, indent=2)
                
        return True
        
    def recommend_parameters(self, n_recommendations=3):
        """
        Get top N parameter recommendations from optimization history
        
        Args:
            n_recommendations: Number of recommendations to return
            
        Returns:
            List of dictionaries with parameter recommendations
        """
        if not self.optimization_history:
            logger.warning("No optimization history available for recommendations")
            return []
            
        # Convert history to DataFrame for easier sorting
        history_df = pd.DataFrame(self.optimization_history)
        
        # Sort by target value (descending if maximizing, ascending if minimizing)
        if self.maximize:
            sorted_df = history_df.sort_values(by=self.target_col, ascending=False)
        else:
            sorted_df = history_df.sort_values(by=self.target_col, ascending=True)
            
        # Get top N parameter sets
        top_n = sorted_df.head(n_recommendations)
        
        # Format results
        recommendations = []
        for _, row in top_n.iterrows():
            rec = {}
            for col in row.index:
                # Only include parameter columns and prediction
                if col not in ['iteration', 'phase']:
                    if col == self.target_col:
                        # Store target as 'prediction'
                        rec['prediction'] = row[col] if self.maximize else -row[col]
                    else:
                        # Store parameter value
                        rec[col] = row[col]
            recommendations.append(rec)
            
        return recommendations
