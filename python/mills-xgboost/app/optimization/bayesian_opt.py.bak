import pandas as pd
import numpy as np
from bayes_opt import BayesianOptimization
# In version 3.0.1, the acquisition functions are directly imported from the package
from bayes_opt import acquisition as bayes_acq
import numpy as np
import logging
import json
import os
import sys
from datetime import datetime

# Standard logger
logger = logging.getLogger(__name__)

# Set up a dedicated logger for Bayesian optimization
bayes_logger = logging.getLogger('bayes_optimization')

# Check if the bayes logger already has handlers to avoid duplicates
if not bayes_logger.handlers:
    # Create logs directory if it doesn't exist
    logs_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'logs')
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
        
    # Create a dedicated log file handler for Bayesian optimization
    bayes_log_file = os.path.join(logs_dir, 'bayes_opt.log')
    file_handler = logging.FileHandler(bayes_log_file, mode='w')
    
    # Define a custom formatter that includes timestamp and level
    formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - BAYES-OPT: %(message)s')
    file_handler.setFormatter(formatter)
    
    # Add the handler to the logger
    bayes_logger.addHandler(file_handler)
    
    # Set the logging level
    bayes_logger.setLevel(logging.DEBUG)
    
    # Prevent propagation to avoid duplicate logs
    bayes_logger.propagate = False
    
    bayes_logger.info('Bayesian optimization logger initialized')
    bayes_logger.info('-' * 80)
    bayes_logger.info('MILLS OPTIMIZATION ENGINE STARTING')
    bayes_logger.info('-' * 80)

# Use the main app's logging configuration for general logs
logger = logging.getLogger(__name__)

class MillBayesianOptimizer:
    """
    Bayesian optimization for mill parameter tuning.
    Uses a trained XGBoost model to find optimal parameter settings.
    """
    
    def __init__(self, xgboost_model, target_col='PSI80', maximize=True):
        """
        Initialize the Bayesian optimizer
        
        Args:
            xgboost_model: Trained XGBoost model instance
            target_col: Target column to optimize (default: 'PSI80')
            maximize: Whether to maximize (True) or minimize (False) the target
        """
        self.model = xgboost_model
        self.target_col = target_col
        self.maximize = maximize
        self.optimizer = None
        self.pbounds = {}
        self.param_constraints = {}
        self.best_params = None
        self.optimization_history = []
        
        logger.info(f"Bayesian optimizer initialized for {'maximizing' if maximize else 'minimizing'} {target_col}")
    
    def _black_box_function(self, **kwargs):
        """
        Black box function that predicts the outcome using the XGBoost model
        
        Args:
            **kwargs: Parameter values to optimize
            
        Returns:
            Predicted target value (negated if minimizing)
        """
        try:
            # Create a dictionary of parameter values from kwargs
            param_values = {k: float(v) for k, v in kwargs.items()}
            data = pd.DataFrame([param_values])
            
            # Check if parameters meet constraints
            if not self._check_constraints(param_values):
                # Return a very poor score (but not infinity) to discourage invalid combinations
                # Use a large finite number instead of infinity
                return -1e10 if self.maximize else 1e10
            
            # Make prediction using the model
            prediction = self.model.predict(data)[0]
            
            # Log this iteration
            self.optimization_history.append({
                'params': param_values,
                'prediction': float(prediction),
                'timestamp': datetime.now().isoformat()
            })
            
            # Return prediction (negative if minimizing)
            return prediction if self.maximize else -prediction
            
        except Exception as e:
            error_msg = f"Error in black box function: {e}"
            logger.error(error_msg)
            bayes_logger.error(error_msg)
            # Return a very bad value in case of error
            return -float('inf') if self.maximize else float('inf')
    
    def set_parameter_bounds(self, pbounds=None, data=None):
        """
        Set the parameter bounds for optimization
        
        Args:
            pbounds: Dictionary mapping parameter names to (min, max) tuples
            data: Optional DataFrame to derive bounds from
            
        Returns:
            Dictionary of parameter bounds
        """
        if pbounds is not None:
            self.pbounds = pbounds
        elif data is not None:
            # Derive bounds from data
            self.pbounds = {}
            for feature in self.model.features:
                if feature in data.columns:
                    # Add a small buffer to min/max values
                    min_val = data[feature].min() * 0.95  # 5% below minimum
                    max_val = data[feature].max() * 1.05  # 5% above maximum
                    self.pbounds[feature] = (min_val, max_val)
        else:
            # Default bounds for common parameters
            self.pbounds = {
                'Ore': (160.0, 200.0),
                'WaterMill': (12.0, 18.0),
                'WaterZumpf': (140.0, 240.0),
                'PressureHC': (0.3, 0.5),
                'DensityHC': (1500, 1800),
                'MotorAmp': (170.0, 220.0)
            }
        
        logger.info(f"Parameter bounds set: {self.pbounds}")
        return self.pbounds
    
    def set_constraints(self, constraints=None):
        """
        Set constraints on parameter combinations
        
        Args:
            constraints: Dictionary of constraint functions
                        Each key is a constraint name, value is a function that
                        takes parameter dictionary and returns True if valid
        """
        self.param_constraints = constraints or {}
        logger.info(f"Set {len(self.param_constraints)} parameter constraints")
    
    def _check_constraints(self, params):
        """
        Check if parameters meet all constraints
        
        Args:
            params: Dictionary of parameter values
            
        Returns:
            True if all constraints are met, False otherwise
        """
        for name, constraint_func in self.param_constraints.items():
            if not constraint_func(params):
                logger.debug(f"Constraint '{name}' not met with params: {params}")
                return False
        return True
    
    def optimize(self, init_points=5, n_iter=25, acq='ei', kappa=2.5, xi=0.0, save_dir=None):
        """
        Run the Bayesian optimization process
        
        Args:
            init_points: Number of random exploration steps before exploitation
            n_iter: Number of optimization iterations
            acq: Acquisition function type ('ei', 'ucb', or 'poi')
            kappa: Parameter for 'ucb' acquisition function
            xi: Parameter for 'ei' and 'poi' acquisition functions
            save_dir: Optional directory to save optimization results
            
        Returns:
            Dictionary with optimization results or error information
        """
        # Log the start of optimization with detailed parameters
        bayes_logger.info("="*80)
        bayes_logger.info(f"STARTING BAYESIAN OPTIMIZATION FOR TARGET: {self.target_col}")
        bayes_logger.info(f"Optimization goal: {'MAXIMIZE' if self.maximize else 'MINIMIZE'} {self.target_col}")
        bayes_logger.info(f"Initial points: {init_points}, Optimization iterations: {n_iter}")
        bayes_logger.info(f"Acquisition function: {acq}, kappa: {kappa}, xi: {xi}")
        bayes_logger.info("="*80)
        
        try:
            # Check if parameter bounds are set
            if not self.pbounds:
                logger.warning("Parameter bounds not set, using defaults")
                self.set_parameter_bounds()
            
            # Initialize optimizer
            self.optimizer = BayesianOptimization(
                f=self._black_box_function,
                pbounds=self.pbounds,
                random_state=42
            )
            
            # Set additional parameters based on acquisition function type
            utility_params = {}
            if acq == 'ucb':
                utility_params = {'kappa': kappa}
            elif acq in ['ei', 'poi']:
                utility_params = {'xi': xi}
                
            logger.info(f"Using acquisition function: {acq} with params: {utility_params}")
            
            # Clear history
            self.optimization_history = []
            
            # Run optimization with enhanced safeguards for numerical stability
            logger.info(f"Starting optimization with {init_points} initial points and {n_iter} iterations")
            
            # Set the utility/acquisition function as a property of the optimizer
            # In the current version 3.0.1, we need to set these parameters differently
            self.optimizer.set_gp_params()
            # Set the acquisition function type and parameters via optimizer attributes
            self.optimizer._acquisition = acq
            self.optimizer._kappa = kappa if acq == 'ucb' else None
            self.optimizer._xi = xi if acq in ['ei', 'poi'] else None
            
            # Define a safe boundary for optimization target values to prevent numerical issues
            # These bounds should be large enough to represent meaningful differences
            # but small enough to avoid numerical issues in the optimization algorithm
            MIN_TARGET = -1e6
            MAX_TARGET = 1e6
            
            # Safe register wrapper to clip values to safe range
            def safe_register(params, target_value, iteration_type, iteration_num, total_iterations):
                # Clip to safe range to avoid numerical instability
                if not isinstance(target_value, (int, float)) or np.isnan(target_value):
                    logger.warning(f"Invalid target value {target_value} for params {params}, using fallback")
                    bayes_logger.warning(f"Invalid target value encountered - using fallback value")
                    target_value = MIN_TARGET if self.maximize else MAX_TARGET
                
                # Clip extreme values
                clipped_target = np.clip(target_value, MIN_TARGET, MAX_TARGET)
                if clipped_target != target_value:
                    logger.warning(f"Clipped extreme target value {target_value} to {clipped_target}")
                    bayes_logger.warning(f"Clipped extreme value {target_value} → {clipped_target}")
                
                try:
                    self.optimizer.register(params=params, target=clipped_target)
                    
                    # Create formatted log messages for the optimization process
                    progress_percent = (iteration_num / total_iterations) * 100
                    
                    # Get current best value for comparison
                    current_best = None
                    if hasattr(self.optimizer, 'max') and self.optimizer.max:
                        current_best = self.optimizer.max.get('target')
                        
                    # Determine if this is a new best value
                    is_new_best = False
                    if current_best is not None:
                        if (self.maximize and clipped_target > current_best) or (not self.maximize and clipped_target < current_best):
                            is_new_best = True
                    
                    # Log optimization step with detailed information
                    step_description = f"[{iteration_type}] Step {iteration_num}/{total_iterations} ({progress_percent:.1f}%):"
                    param_details = ", ".join([f"{k}={v:.3f}" for k, v in params.items()])
                    result_details = f"{self.target_col}={clipped_target:.4f}"
                    improvement = "✓ NEW BEST!" if is_new_best else ""
                    
                    # Log to the dedicated bayes_opt.log file
                    bayes_logger.info(f"{step_description} {param_details} → {result_details} {improvement}")
                    
                    # Log more detailed information at debug level
                    bayes_logger.debug(f"Full parameters: {params}")
                    
                    # Regular app logging at a less detailed level
                    logger.debug(f"Registered params: {params}, target: {clipped_target}")
                    
                except Exception as e:
                    error_msg = f"Error in register: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
                    raise
                
                return clipped_target
            
            # Log parameter bounds being used
            bayes_logger.info("Parameter bounds:")
            for param_name, bounds in self.pbounds.items():
                bayes_logger.info(f"  {param_name}: [{bounds[0]:.3f}, {bounds[1]:.3f}]")
                
            # Log any constraints
            if self.param_constraints:
                bayes_logger.info("Parameter constraints:")
                for constraint_name in self.param_constraints.keys():
                    bayes_logger.info(f"  {constraint_name}")
            
            bayes_logger.info("-"*60)
            bayes_logger.info("PHASE 1: EXPLORATION (Random Sampling)")
            bayes_logger.info("-"*60)
                
            # Initialize with default failure values in case all steps fail
            all_steps_failed = True
            default_bad_value = MIN_TARGET if self.maximize else MAX_TARGET
            
            # Execute exploration phase with random points
            for i in range(init_points):
                try:
                    # Random exploration
                    next_point = self.optimizer.suggest()
                    target = self._black_box_function(**next_point)
                    clipped_target = safe_register(next_point, target, "EXPLORE", i+1, init_points)
                    logger.info(f"Exploration step {i+1}/{init_points}, target: {clipped_target:.4f}")
                    
                    # At least one step succeeded
                    if clipped_target != default_bad_value:
                        all_steps_failed = False
                        
                except Exception as e:
                    error_msg = f"Error in exploration step {i+1}: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
            
            bayes_logger.info("-"*60)
            bayes_logger.info("PHASE 2: EXPLOITATION (Guided Optimization)")
            bayes_logger.info("-"*60)
            
            # Execute exploitation phase with guided optimization
            for i in range(n_iter):
                try:
                    # Guided optimization
                    next_point = self.optimizer.suggest()
                    target = self._black_box_function(**next_point)
                    clipped_target = safe_register(next_point, target, "OPTIMIZE", i+1, n_iter)
                    logger.info(f"Optimization step {i+1}/{n_iter}, target: {clipped_target:.4f}")
                    
                    # At least one step succeeded
                    if clipped_target != default_bad_value:
                        all_steps_failed = False
                        
                except Exception as e:
                    error_msg = f"Error in optimization step {i+1}: {str(e)}"
                    logger.error(error_msg)
                    bayes_logger.error(error_msg)
                    
            # If all steps failed, raise an exception
            if all_steps_failed:
                error_msg = "All optimization steps failed. Check the logs for details."
                logger.error(error_msg)
                bayes_logger.error(error_msg)
                bayes_logger.error("OPTIMIZATION PROCESS TERMINATED - ALL STEPS FAILED")
                raise ValueError(error_msg)
            
            # Check if we have valid optimization results
            if hasattr(self.optimizer, 'max') and self.optimizer.max:
                # Get the best parameters and target value
                self.best_params = self.optimizer.max['params']
                best_target = self.optimizer.max['target']
                
                # Calculate actual prediction for the best parameters
                best_prediction = best_target if self.maximize else -best_target
                
                logger.info(f"Optimization complete. Best target: {best_prediction:.4f}")
            else:
                # This shouldn't happen due to the all_steps_failed check above, but just in case
                error_msg = "No valid optimization results found"
                logger.error(error_msg)
                bayes_logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Log the optimization results in detail
            bayes_logger.info("-"*60)
            bayes_logger.info("OPTIMIZATION COMPLETE")
            bayes_logger.info("-"*60)
            bayes_logger.info(f"Best {self.target_col} value: {best_target:.6f} ({'maximized' if self.maximize else 'minimized'})")
            bayes_logger.info("Best parameter combination:")
            for param, value in self.best_params.items():
                bayes_logger.info(f"  {param}: {value:.4f}")
            
            # Show improvement over baseline if available
            if hasattr(self, 'baseline_performance') and self.baseline_performance is not None:
                improvement = best_target - self.baseline_performance if self.maximize else self.baseline_performance - best_target
                percent_improvement = (improvement / abs(self.baseline_performance)) * 100 if self.baseline_performance != 0 else float('inf')
                bayes_logger.info(f"Improvement over baseline: {improvement:.4f} ({percent_improvement:.2f}%)")
            
            # Save results if directory is provided
            if save_dir:
                self.save_results(save_dir)
                bayes_logger.info(f"Results saved to: {save_dir}")
            
            # Log the final recommendation
            recommendations = self.recommend_parameters(n_recommendations=3)
            bayes_logger.info("-"*60)
            bayes_logger.info("TOP RECOMMENDATIONS:")
            for i, rec in enumerate(recommendations):
                bayes_logger.info(f"Recommendation #{i+1} (Expected {self.target_col}: {rec['prediction']:.4f}):")
                for param, value in {k: v for k, v in rec.items() if k != 'prediction'}.items():
                    bayes_logger.info(f"  {param}: {value:.4f}")
            
            # Return optimization results
            return {
                "best_params": self.best_params,
                "best_target": best_target,
                "maximize": self.maximize,
                "target_col": self.target_col,
                "history": self.optimization_history
            }
            
        except Exception as e:
            error_msg = f"Error during optimization: {e}"
            logger.error(error_msg)
            bayes_logger.error(error_msg)
            bayes_logger.error("OPTIMIZATION PROCESS TERMINATED WITH ERROR")
            return {"error": str(e)}
            
            return clipped_target
        
        # Log parameter bounds being used
        bayes_logger.info("Parameter bounds:")
        for param_name, bounds in self.pbounds.items():
            bayes_logger.info(f"  {param_name}: [{bounds[0]:.3f}, {bounds[1]:.3f}]")
        
        # Log any constraints
        if self.param_constraints:
            bayes_logger.info("Parameter constraints:")
            for constraint_name in self.param_constraints.keys():
                bayes_logger.info(f"  {constraint_name}")
        
        bayes_logger.info("-"*60)
        bayes_logger.info("PHASE 1: EXPLORATION (Random Sampling)")
        bayes_logger.info("-"*60)
            
        # Initialize with default failure values in case all steps fail
        all_steps_failed = True
        default_bad_value = MIN_TARGET if self.maximize else MAX_TARGET
        
        # Execute exploration phase with random points
        for i in range(init_points):
            try:
                # Random exploration
                next_point = self.optimizer.suggest()
                target = self._black_box_function(**next_point)
                clipped_target = safe_register(next_point, target, "EXPLORE", i+1, init_points)
                logger.info(f"Exploration step {i+1}/{init_points}, target: {clipped_target:.4f}")
                
                # At least one step succeeded
                if clipped_target != default_bad_value:
                    all_steps_failed = False
                    
            except Exception as e:
                error_msg = f"Error in exploration step {i+1}: {str(e)}"
                logger.error(error_msg)
                bayes_logger.error(error_msg)
        
        bayes_logger.info("-"*60)
        bayes_logger.info("PHASE 2: EXPLOITATION (Guided Optimization)")
        bayes_logger.info("-"*60)
        
        # Execute exploitation phase with guided optimization
        for i in range(n_iter):
            try:
                # Guided optimization
                next_point = self.optimizer.suggest()
                target = self._black_box_function(**next_point)
                clipped_target = safe_register(next_point, target, "OPTIMIZE", i+1, n_iter)
                logger.info(f"Optimization step {i+1}/{n_iter}, target: {clipped_target:.4f}")
                
                # At least one step succeeded
                if clipped_target != default_bad_value:
                    all_steps_failed = False
                    
            except Exception as e:
                error_msg = f"Error in optimization step {i+1}: {str(e)}"
                logger.error(error_msg)
                bayes_logger.error(error_msg)
                
        # If all steps failed, raise an exception
        if all_steps_failed:
            error_msg = "All optimization steps failed. Check the logs for details."
            logger.error(error_msg)
            bayes_logger.error(error_msg)
            bayes_logger.error("OPTIMIZATION PROCESS TERMINATED - ALL STEPS FAILED")
            raise ValueError(error_msg)
        
        # Check if we have valid optimization results
        if hasattr(self.optimizer, 'max') and self.optimizer.max:
            # Get the best parameters and target value
            self.best_params = self.optimizer.max['params']
            best_target = self.optimizer.max['target']
            
            # Calculate actual prediction for the best parameters
            best_prediction = best_target if self.maximize else -best_target
        else:
            # This shouldn't happen due to the all_steps_failed check above, but just in case
            error_msg = "No valid optimization results found"
            logger.error(error_msg)
            bayes_logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Log the optimization results in detail
        bayes_logger.info("-"*60)
        bayes_logger.info("OPTIMIZATION COMPLETE")
        bayes_logger.info("-"*60)
        bayes_logger.info(f"Best {self.target_col} value: {best_target:.6f} ({'maximized' if self.maximize else 'minimized'})")
        bayes_logger.info("Best parameter combination:")
        for param, value in self.best_params.items():
            bayes_logger.info(f"  {param}: {value:.4f}")
        
        # Show improvement over baseline if available
        if hasattr(self, 'baseline_performance') and self.baseline_performance is not None:
            improvement = best_target - self.baseline_performance if self.maximize else self.baseline_performance - best_target
            percent_improvement = (improvement / abs(self.baseline_performance)) * 100 if self.baseline_performance != 0 else float('inf')
            bayes_logger.info(f"Improvement over baseline: {improvement:.4f} ({percent_improvement:.2f}%)")
        
        # Save results if directory is provided
        if save_dir:
            self.save_results(save_dir)
            bayes_logger.info(f"Results saved to: {save_dir}")
        
        # Log the final recommendations
        recommendations = self.recommend_parameters(n_recommendations=3)
        bayes_logger.info("-"*60)
        bayes_logger.info("TOP RECOMMENDATIONS:")
        for i, rec in enumerate(recommendations):
            bayes_logger.info(f"Recommendation #{i+1} (Expected {self.target_col}: {rec['prediction']:.4f}):")
            for param, value in {k: v for k, v in rec.items() if k != 'prediction'}.items():
                bayes_logger.info(f"  {param}: {value:.4f}")
        
        # Return optimization results
        return {
                "best_params": self.best_params,
                "best_target": best_target,
                "maximize": self.maximize,
                "target_col": self.target_col,
                "history": self.optimization_history
            }
            
        except Exception as e:
            logger.error(f"Error during optimization: {e}")
            raise
    
    def _save_optimization_results(self, directory):
        """
        Save optimization results to disk
        
        Args:
            directory: Directory to save results
        """
        os.makedirs(directory, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(directory, f"optimization_results_{timestamp}.json")
        
        results = {
            'target_col': self.target_col,
            'maximize': self.maximize,
            'bounds': self.pbounds,
            'best_params': self.best_params,
            'best_target': self.optimizer.max['target'],
            'history': self.optimization_history,
            'timestamp': timestamp
        }
        
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Optimization results saved to {filename}")
    
    def recommend_parameters(self, n_recommendations=3):
        """
        Get top N parameter recommendations from optimization history
        
        Args:
            n_recommendations: Number of recommendations to return
            
        Returns:
            List of dictionaries with parameter recommendations
        """
        if not self.optimization_history:
            logger.warning("No optimization history available")
            bayes_logger.warning("Cannot provide recommendations: No optimization history available")
            return []
        
        # Log that we're generating recommendations
        bayes_logger.info(f"Generating top {n_recommendations} parameter recommendations")
        
        # Convert history to DataFrame
        df = pd.DataFrame([
            {**item['params'], 'prediction': item['prediction']}
            for item in self.optimization_history
        ])
        
        # Sort by prediction (high to low if maximizing, low to high if minimizing)
        ascending = not self.maximize
        df_sorted = df.sort_values('prediction', ascending=ascending)
        
        bayes_logger.debug(f"Total optimization points evaluated: {len(df)}")
        bayes_logger.debug(f"Range of {self.target_col} values: [{df['prediction'].min():.4f}, {df['prediction'].max():.4f}]")
        
        # Get top N recommendations
        top_n = df_sorted.head(n_recommendations)
        
        # Format recommendations
        recommendations = []
        for _, row in top_n.iterrows():
            params = {col: row[col] for col in row.index if col != 'prediction'}
            recommendations.append({
                'params': params,
                'predicted_value': row['prediction']
            })
        
        return recommendations
